{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Replication Project\n",
    "\n",
    "This notebook allows you to run the Vision Transformer (ViT) replication project on Google Colab.\n",
    "\n",
    "## Setup\n",
    "First, we need to ensure we have the necessary dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install timm tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "We define the Vision Transformer architecture components: PatchEmbedding, MultiHeadAttention, MLP, TransformerBlock, and the main VisionTransformer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image into patch embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # create patch embeddings by conv2d\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch_size, Channel, Height, Width] -> [Batch_size, embed_dim, H', W']\n",
    "        x = self.proj(x) \n",
    "        Batch_size, Channel, Height, Width = x.shape\n",
    "        # [Batch_size, embed_dim, H', W'] -> [Batch_size, num_patches, embed_dim]\n",
    "        x = x.flatten(2).transpose(1, 2)  \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention\"\"\"\n",
    "    \n",
    "    # embed_dim must be divisible by num_heads\n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch_size, 197, 768]\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        # [Batch_size, 197, 768] -> [Batch_size, 197, 3, 12, 64] -> [3, Batch_size, 12, 197, 64]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        # [3, Batch_size, 12, 197, 64] -> [Batch_size, 12, 197, 64]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        # [Batch_size, 12, 197, 64] -> [Batch_size, 12, 197, 197]\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # transfer attention to values\n",
    "        # [Batch_size, 12, 197, 197] -> [Batch_size, 197, 768]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward network\n",
    "    embed_dim -> hidden_dim(expand mlp_ratio times) -> embed_dim\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Block Only Encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch_size, num_patches+1, embed_dim]\n",
    "        # residual connection\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # residual connection\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer Model\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.0,\n",
    "        emb_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Patch embedding [Batch_size, num_patches, embed_dim]\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Learnable class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embedding \n",
    "        # num_patches + 1  including the class token\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Dropout for embeddings\n",
    "        self.pos_dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.normal_(m.bias, std=1e-6)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        # [Batch_size, Channel, Height, Width] -> [Batch_size, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  \n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(Batch_size, -1, -1)  # [Batch_size, 1, embed_dim]\n",
    "        \n",
    "        # [Batch_size, num_patches, embed_dim] -> [Batch_size, num_patches+1, embed_dim]\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  \n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        # [Batch_size, num_patches+1, embed_dim] -> [Batch_size, num_patches+1, embed_dim]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # use class tokens for classification\n",
    "        # [Batch_size, num_patches+1, embed_dim] -> [Batch_size, embed_dim]\n",
    "        cls_token_final = x[:, 0]  \n",
    "\n",
    "        # [Batch_size, embed_dim] -> [Batch_size, num_classes]\n",
    "        class_vectors = self.head(cls_token_final)  \n",
    "        \n",
    "        return class_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Functions for loading pretrained weights and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, timm_model_name, num_classes):\n",
    "    \"\"\"Load pretrained weights from timm library\"\"\"\n",
    "    print(f\"Loading pretrained weights from timm: {timm_model_name}\")\n",
    "    \n",
    "    # Load pretrained model from timm\n",
    "    pretrained_model = timm.create_model(timm_model_name, pretrained=True, num_classes=1000)\n",
    "    \n",
    "    # Get state dicts\n",
    "    our_state_dict = model.state_dict()\n",
    "    pretrained_state_dict = pretrained_model.state_dict()\n",
    "    \n",
    "    # Debug: Print key names to understand the structure\n",
    "    print(f\"\\nDebug: Our model has {len(our_state_dict)} parameters\")\n",
    "    print(f\"Debug: Pretrained model has {len(pretrained_state_dict)} parameters\")\n",
    "    print(f\"Debug: Sample our keys (first 10):\")\n",
    "    for i, key in enumerate(list(our_state_dict.keys())[:10]):\n",
    "        print(f\"  {i+1}. {key}\")\n",
    "    print(f\"Debug: Sample pretrained keys (first 10):\")\n",
    "    for i, key in enumerate(list(pretrained_state_dict.keys())[:10]):\n",
    "        print(f\"  {i+1}. {key}\")\n",
    "    \n",
    "    # Create a mapping from our keys to pretrained keys\n",
    "    # timm ViT models typically use the same structure, so keys should match\n",
    "    key_mapping = {}\n",
    "    for our_key in our_state_dict.keys():\n",
    "        # Try direct match first (most common case)\n",
    "        if our_key in pretrained_state_dict:\n",
    "            key_mapping[our_key] = our_key\n",
    "        else:\n",
    "            # If direct match fails, try to find the corresponding key\n",
    "            # This handles cases where timm might use slightly different naming\n",
    "            found_match = False\n",
    "            \n",
    "            # Extract the key parts\n",
    "            our_parts = our_key.split('.')\n",
    "            \n",
    "            # Try to find matching key in pretrained model\n",
    "            for pretrained_key in pretrained_state_dict.keys():\n",
    "                pretrained_parts = pretrained_key.split('.')\n",
    "                \n",
    "                # Match if the last parts are the same and structure is similar\n",
    "                if len(our_parts) == len(pretrained_parts):\n",
    "                    # Check if the last 2-3 parts match (handles blocks.0.attn vs blocks.0.attn)\n",
    "                    if our_parts[-1] == pretrained_parts[-1]:\n",
    "                        # For blocks, also check the layer number and component type\n",
    "                        if 'blocks' in our_key and 'blocks' in pretrained_key:\n",
    "                            # Extract block number and component\n",
    "                            our_block_idx = None\n",
    "                            pretrained_block_idx = None\n",
    "                            for i, part in enumerate(our_parts):\n",
    "                                if part == 'blocks' and i+1 < len(our_parts):\n",
    "                                    try:\n",
    "                                        our_block_idx = int(our_parts[i+1])\n",
    "                                        break\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            for i, part in enumerate(pretrained_parts):\n",
    "                                if part == 'blocks' and i+1 < len(pretrained_parts):\n",
    "                                    try:\n",
    "                                        pretrained_block_idx = int(pretrained_parts[i+1])\n",
    "                                        break\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            \n",
    "                            # Check if block indices match and component types match\n",
    "                            if our_block_idx == pretrained_block_idx:\n",
    "                                # Check component type (attn, mlp, norm)\n",
    "                                our_comp = '.'.join(our_parts[our_parts.index('blocks')+2:])\n",
    "                                pretrained_comp = '.'.join(pretrained_parts[pretrained_parts.index('blocks')+2:])\n",
    "                                if our_comp == pretrained_comp:\n",
    "                                    key_mapping[our_key] = pretrained_key\n",
    "                                    found_match = True\n",
    "                                    break\n",
    "                        elif our_parts[-2:] == pretrained_parts[-2:]:\n",
    "                            # For non-block keys, match last 2 parts\n",
    "                            key_mapping[our_key] = pretrained_key\n",
    "                            found_match = True\n",
    "                            break\n",
    "    \n",
    "    # Try to load matching weights\n",
    "    loaded_keys = []\n",
    "    missing_keys = []\n",
    "    shape_mismatch_keys = []\n",
    "    \n",
    "    for our_key in our_state_dict.keys():\n",
    "        pretrained_key = key_mapping.get(our_key, our_key)\n",
    "        \n",
    "        if pretrained_key in pretrained_state_dict:\n",
    "            if our_state_dict[our_key].shape == pretrained_state_dict[pretrained_key].shape:\n",
    "                our_state_dict[our_key] = pretrained_state_dict[pretrained_key]\n",
    "                loaded_keys.append(our_key)\n",
    "            else:\n",
    "                shape_mismatch_keys.append(f\"{our_key} (our: {our_state_dict[our_key].shape} vs pretrained: {pretrained_state_dict[pretrained_key].shape})\")\n",
    "        else:\n",
    "            missing_keys.append(our_key)\n",
    "    \n",
    "    # Handle classification head separately\n",
    "    if 'head.weight' in pretrained_state_dict and 'head.bias' in pretrained_state_dict:\n",
    "        pretrained_head_weight = pretrained_state_dict['head.weight']\n",
    "        pretrained_head_bias = pretrained_state_dict['head.bias']\n",
    "        \n",
    "        if num_classes == 1000:\n",
    "            if 'head.weight' not in loaded_keys:\n",
    "                our_state_dict['head.weight'] = pretrained_head_weight\n",
    "                our_state_dict['head.bias'] = pretrained_head_bias\n",
    "                loaded_keys.extend(['head.weight', 'head.bias'])\n",
    "        else:\n",
    "            print(f\"Note: Re-initializing head for {num_classes} classes (pretrained had {pretrained_head_weight.shape[0]})\")\n",
    "    \n",
    "    # Load the state dict\n",
    "    model.load_state_dict(our_state_dict, strict=False)\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(loaded_keys)} pretrained layers\")\n",
    "    if missing_keys:\n",
    "        print(f\"Warning: {len(missing_keys)} keys not found in pretrained model (first 5): {missing_keys[:5]}\")\n",
    "    if shape_mismatch_keys:\n",
    "        print(f\"Warning: {len(shape_mismatch_keys)} layers have shape mismatches (first 3):\")\n",
    "        for mismatch in shape_mismatch_keys[:3]:\n",
    "            print(f\"  {mismatch}\")\n",
    "    \n",
    "    # Critical check: verify key layers were loaded\n",
    "    critical_keys = ['patch_embed.proj.weight', 'pos_embed', 'cls_token']\n",
    "    print(f\"\\nCritical layer check:\")\n",
    "    for key in critical_keys:\n",
    "        if key in loaded_keys:\n",
    "            print(f\"  ✓ Loaded: {key}\")\n",
    "        else:\n",
    "            print(f\"  ✗ WARNING: NOT loaded: {key}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_dataloader(batch_size=32, num_workers=2, img_size=224, train=False):\n",
    "    \"\"\"Get DataLoader for CIFAR-10\"\"\"\n",
    "    \n",
    "    # Transform: Resize to 224x224 for ViT, ImageNet normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    root = './data'\n",
    "    dataset = datasets.CIFAR10(root=root, train=train, download=True, transform=transform)\n",
    "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return loader, classes\n",
    "\n",
    "def evaluate_model(model, data_loader, device, classes):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    acc = 100. * correct / total\n",
    "    print(f'Accuracy: {acc:.2f}%')\n",
    "    \n",
    "    # Additional diagnostic info\n",
    "    if acc < 15.0:  # Very low accuracy\n",
    "        print(f'\\n⚠️  WARNING: Very low accuracy ({acc:.2f}%)!')\n",
    "        print('This is likely because:')\n",
    "        print('1. Classification head is randomly initialized (expected if num_classes != 1000)')\n",
    "        print('2. Model needs fine-tuning on CIFAR10 before evaluation')\n",
    "        print('3. Random guessing would give ~10% accuracy on CIFAR10')\n",
    "        print('\\nTo fix: You need to fine-tune the model on CIFAR10 training set first!')\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def verify_weight_loading(model, timm_model_name):\n",
    "    \"\"\"Verify that critical weights were loaded correctly\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Weight Loading Verification\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load timm model for comparison\n",
    "    pretrained_model = timm.create_model(timm_model_name, pretrained=True, num_classes=1000)\n",
    "    pretrained_state_dict = pretrained_model.state_dict()\n",
    "    our_state_dict = model.state_dict()\n",
    "    \n",
    "    # Check critical layers\n",
    "    critical_layers = {\n",
    "        'patch_embed.proj.weight': 'Patch embedding projection',\n",
    "        'pos_embed': 'Positional embeddings',\n",
    "        'cls_token': 'Class token',\n",
    "        'norm.weight': 'Final layer norm weight',\n",
    "        'blocks.0.norm1.weight': 'First transformer block norm'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCritical layer verification:\")\n",
    "    all_loaded = True\n",
    "    for key, description in critical_layers.items():\n",
    "        if key in our_state_dict and key in pretrained_state_dict:\n",
    "            our_weight = our_state_dict[key]\n",
    "            pretrained_weight = pretrained_state_dict[key]\n",
    "            \n",
    "            if torch.allclose(our_weight, pretrained_weight, atol=1e-5):\n",
    "                print(f\"  ✓ {description} ({key}): CORRECTLY LOADED\")\n",
    "            else:\n",
    "                print(f\"  ✗ {description} ({key}): MISMATCH!\")\n",
    "                print(f\"    Our: {our_weight.shape}, Pretrained: {pretrained_weight.shape}\")\n",
    "                all_loaded = False\n",
    "        else:\n",
    "            print(f\"  ✗ {description} ({key}): NOT FOUND\")\n",
    "            all_loaded = False\n",
    "    \n",
    "    if all_loaded:\n",
    "        print(\"\\n✓ All critical layers loaded correctly!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Some critical layers may not be loaded correctly!\")\n",
    "    \n",
    "    return all_loaded\n",
    "\n",
    "def get_dataloader_with_augmentation(batch_size=32, num_workers=2, img_size=224, train=True):\n",
    "    \"\"\"Get DataLoader with data augmentation for training CIFAR-10\"\"\"\n",
    "    \n",
    "    if train:\n",
    "        # Training: use data augmentation\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomCrop(img_size, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        # Validation/Test: no augmentation\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    root = './data'\n",
    "    dataset = datasets.CIFAR10(root=root, train=train, download=True, transform=transform)\n",
    "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=train,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return loader, classes\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [Train]')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='[Val]')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def fine_tune_model(\n",
    "    model,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    freeze_backbone=False,\n",
    "    device='cuda',\n",
    "    save_path='best_model.pth'\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune the model on CIFAR10\n",
    "    \n",
    "    Args:\n",
    "        model: The VisionTransformer model\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        learning_rate: Learning rate\n",
    "        weight_decay: Weight decay for optimizer\n",
    "        freeze_backbone: If True, only train the classification head\n",
    "        device: Device to train on\n",
    "        save_path: Path to save the best model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fine-tuning on CIFAR10\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Freeze backbone: {freeze_backbone}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Freeze backbone if requested\n",
    "    if freeze_backbone:\n",
    "        print(\"Freezing backbone, only training classification head...\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'head' not in name:\n",
    "                param.requires_grad = False\n",
    "        # Count trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100.*trainable_params/total_params:.2f}%)\")\n",
    "    else:\n",
    "        print(\"Training entire model (end-to-end fine-tuning)...\")\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Get data loaders\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    train_loader, classes = get_dataloader_with_augmentation(\n",
    "        batch_size=batch_size, num_workers=2, train=True\n",
    "    )\n",
    "    val_loader, _ = get_dataloader_with_augmentation(\n",
    "        batch_size=batch_size, num_workers=2, train=False\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use different learning rates for backbone and head if not freezing\n",
    "    if freeze_backbone:\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        # Use smaller LR for backbone, larger for head\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'head' in name:\n",
    "                    head_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': learning_rate * 0.1},  # Smaller LR for pretrained\n",
    "            {'params': head_params, 'lr': learning_rate}  # Normal LR for new head\n",
    "        ], weight_decay=weight_decay)\n",
    "        print(f\"Using different LRs: backbone={learning_rate*0.1}, head={learning_rate}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs, eta_min=learning_rate * 0.01\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    \n",
    "    print(f\"\\nStarting training...\\n\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch}/{epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'train_acc': train_acc,\n",
    "            }, save_path)\n",
    "            print(f\"  ✓ Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Run the evaluation on CIFAR-10 and/or CIFAR-100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题诊断\n",
    "\n",
    "如果准确率很低（如7-10%），可能的原因：\n",
    "\n",
    "1. **分类头随机初始化**：当类别数从1000变为10时，分类头被重新初始化。**必须进行微调（fine-tuning）才能获得好的性能**。\n",
    "\n",
    "2. **权重加载不完整**：检查上面的调试输出，确认关键层（patch_embed, pos_embed, cls_token）是否正确加载。\n",
    "\n",
    "3. **模型未设置为评估模式**：确保在评估时调用 `model.eval()`。\n",
    "\n",
    "**解决方案**：\n",
    "- 如果只是测试预训练模型，应该使用 `num_classes=1000` 并在ImageNet上测试\n",
    "- 如果要在CIFAR10上使用，**必须进行微调训练**，不能直接评估\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning on CIFAR10\n",
    "\n",
    "Fine-tune the pretrained ViT model on CIFAR10. You can choose to:\n",
    "- **Freeze backbone**: Only train the classification head (faster, less memory)\n",
    "- **End-to-end**: Train the entire model (slower, better performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Configuration\n",
    "model_name = 'vit_base_patch16_224'\n",
    "num_classes = 10  # CIFAR10 has 10 classes\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 10              # Number of training epochs\n",
    "batch_size = 64           # Batch size (adjust based on GPU memory)\n",
    "learning_rate = 1e-4      # Learning rate\n",
    "weight_decay = 0.01       # Weight decay for regularization\n",
    "freeze_backbone = False   # Set to True to only train classification head\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating model {model_name} for {num_classes} classes...\")\n",
    "model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0\n",
    ")\n",
    "\n",
    "# Load pretrained weights\n",
    "model = load_pretrained_weights(model, model_name, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Fine-tune the model\n",
    "history = fine_tune_model(\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    freeze_backbone=freeze_backbone,\n",
    "    device=device,\n",
    "    save_path='vit_cifar10_best.pth'\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed!\")\n",
    "print(f\"Best validation accuracy: {history['best_val_acc']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model\n",
    "\n",
    "Load the best model and evaluate on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and evaluate\n",
    "checkpoint_path = 'vit_cifar10_best.pth'\n",
    "num_classes = 10  # CIFAR10 has 10 classes\n",
    "batch_size = 64   # Batch size for evaluation\n",
    "\n",
    "# Create model (same architecture)\n",
    "model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Validation accuracy when saved: {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Evaluating on test set...\")\n",
    "print(f\"{'='*60}\")\n",
    "test_loader, classes = get_dataloader(batch_size=batch_size, num_workers=2, train=False)\n",
    "test_acc = evaluate_model(model, test_loader, device, classes)\n",
    "print(f\"\\nFinal test accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for evaluating pretrained model (without fine-tuning)\n",
    "batch_size = 64           # Adjust based on Colab GPU memory (e.g. 32, 64, 128)\n",
    "model_name = 'vit_base_patch16_224'\n",
    "num_classes = 10          # CIFAR10 has 10 classes\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\n{'='*20} Evaluating on CIFAR10 {'='*20}\")\n",
    "print(f\"Creating model {model_name} for {num_classes} classes...\")\n",
    "model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model = load_pretrained_weights(model, model_name, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading CIFAR10 test set...\")\n",
    "# Ensure we're running with appropriate num_workers for Colab\n",
    "loader, classes = get_dataloader(batch_size=batch_size, num_workers=2, train=False)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, loader, device, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
