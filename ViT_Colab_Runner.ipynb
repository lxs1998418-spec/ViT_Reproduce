{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D31UezemjT-"
      },
      "source": [
        "# ViT Replication Project\n",
        "\n",
        "This notebook allows you to run the Vision Transformer (ViT) replication project on Google Colab.\n",
        "\n",
        "## Setup\n",
        "First, we need to ensure we have the necessary dependencies installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lh2pWnKVmjUA",
        "outputId": "1228c838-c642-4861-ba8c-3cc52522be99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install timm tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KsZsu4nGmjUA"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDU90K-JmjUA"
      },
      "source": [
        "## Model Definition\n",
        "We define the Vision Transformer architecture components: PatchEmbedding, MultiHeadAttention, MLP, TransformerBlock, and the main VisionTransformer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S5FAg5oUmjUB"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Convert image into patch embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # create patch embeddings by conv2d\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch_size, Channel, Height, Width] -> [Batch_size, embed_dim, H', W']\n",
        "        x = self.proj(x)\n",
        "        Batch_size, Channel, Height, Width = x.shape\n",
        "        # [Batch_size, embed_dim, H', W'] -> [Batch_size, num_patches, embed_dim]\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Self-Attention\"\"\"\n",
        "\n",
        "    # embed_dim must be divisible by num_heads\n",
        "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch_size, 197, 768]\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        # [Batch_size, 197, 768] -> [Batch_size, 197, 3, 12, 64] -> [3, Batch_size, 12, 197, 64]\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        # [3, Batch_size, 12, 197, 64] -> [Batch_size, 12, 197, 64]\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        # [Batch_size, 12, 197, 64] -> [Batch_size, 12, 197, 197]\n",
        "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # transfer attention to values\n",
        "        # [Batch_size, 12, 197, 197] -> [Batch_size, 197, 768]\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-forward network\n",
        "    embed_dim -> hidden_dim(expand mlp_ratio times) -> embed_dim\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim=768, mlp_ratio=4.0, dropout=0.0):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(embed_dim * mlp_ratio)\n",
        "\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer Block Only Encoder\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [Batch_size, num_patches+1, embed_dim]\n",
        "        # residual connection\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        # residual connection\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer Model\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=1000,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.0,\n",
        "        emb_dropout=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch embedding [Batch_size, num_patches, embed_dim]\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.n_patches\n",
        "\n",
        "        # Learnable class token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Positional embedding\n",
        "        # num_patches + 1  including the class token\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        # Dropout for embeddings\n",
        "        self.pos_dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.normal_(m.bias, std=1e-6)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "        nn.init.normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Batch_size = x.shape[0]\n",
        "\n",
        "        # Patch embedding\n",
        "        # [Batch_size, Channel, Height, Width] -> [Batch_size, num_patches, embed_dim]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add class token\n",
        "        cls_tokens = self.cls_token.expand(Batch_size, -1, -1)  # [Batch_size, 1, embed_dim]\n",
        "\n",
        "        # [Batch_size, num_patches, embed_dim] -> [Batch_size, num_patches+1, embed_dim]\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Add positional embedding\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_dropout(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        # [Batch_size, num_patches+1, embed_dim] -> [Batch_size, num_patches+1, embed_dim]\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Final layer norm\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # use class tokens for classification\n",
        "        # [Batch_size, num_patches+1, embed_dim] -> [Batch_size, embed_dim]\n",
        "        cls_token_final = x[:, 0]\n",
        "\n",
        "        # [Batch_size, embed_dim] -> [Batch_size, num_classes]\n",
        "        class_vectors = self.head(cls_token_final)\n",
        "\n",
        "        return class_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaHK-Rv-mjUB"
      },
      "source": [
        "## Helper Functions\n",
        "Functions for loading pretrained weights and data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "coBkXwhomjUB"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_weights(model, timm_model_name, num_classes):\n",
        "    \"\"\"Load pretrained weights from timm library\"\"\"\n",
        "    print(f\"Loading pretrained weights from timm: {timm_model_name}\")\n",
        "\n",
        "    # Load pretrained model from timm\n",
        "    pretrained_model = timm.create_model(timm_model_name, pretrained=True, num_classes=1000)\n",
        "\n",
        "    # Get state dicts\n",
        "    our_state_dict = model.state_dict()\n",
        "    pretrained_state_dict = pretrained_model.state_dict()\n",
        "\n",
        "    # Debug: Print key names to understand the structure\n",
        "    print(f\"\\nDebug: Our model has {len(our_state_dict)} parameters\")\n",
        "    print(f\"Debug: Pretrained model has {len(pretrained_state_dict)} parameters\")\n",
        "    print(f\"Debug: Sample our keys (first 10):\")\n",
        "    for i, key in enumerate(list(our_state_dict.keys())[:10]):\n",
        "        print(f\"  {i+1}. {key}\")\n",
        "    print(f\"Debug: Sample pretrained keys (first 10):\")\n",
        "    for i, key in enumerate(list(pretrained_state_dict.keys())[:10]):\n",
        "        print(f\"  {i+1}. {key}\")\n",
        "\n",
        "    # Create a mapping from our keys to pretrained keys\n",
        "    # timm ViT models typically use the same structure, so keys should match\n",
        "    key_mapping = {}\n",
        "    for our_key in our_state_dict.keys():\n",
        "        # Try direct match first (most common case)\n",
        "        if our_key in pretrained_state_dict:\n",
        "            key_mapping[our_key] = our_key\n",
        "        else:\n",
        "            # If direct match fails, try to find the corresponding key\n",
        "            # This handles cases where timm might use slightly different naming\n",
        "            found_match = False\n",
        "\n",
        "            # Extract the key parts\n",
        "            our_parts = our_key.split('.')\n",
        "\n",
        "            # Try to find matching key in pretrained model\n",
        "            for pretrained_key in pretrained_state_dict.keys():\n",
        "                pretrained_parts = pretrained_key.split('.')\n",
        "\n",
        "                # Match if the last parts are the same and structure is similar\n",
        "                if len(our_parts) == len(pretrained_parts):\n",
        "                    # Check if the last 2-3 parts match (handles blocks.0.attn vs blocks.0.attn)\n",
        "                    if our_parts[-1] == pretrained_parts[-1]:\n",
        "                        # For blocks, also check the layer number and component type\n",
        "                        if 'blocks' in our_key and 'blocks' in pretrained_key:\n",
        "                            # Extract block number and component\n",
        "                            our_block_idx = None\n",
        "                            pretrained_block_idx = None\n",
        "                            for i, part in enumerate(our_parts):\n",
        "                                if part == 'blocks' and i+1 < len(our_parts):\n",
        "                                    try:\n",
        "                                        our_block_idx = int(our_parts[i+1])\n",
        "                                        break\n",
        "                                    except:\n",
        "                                        pass\n",
        "                            for i, part in enumerate(pretrained_parts):\n",
        "                                if part == 'blocks' and i+1 < len(pretrained_parts):\n",
        "                                    try:\n",
        "                                        pretrained_block_idx = int(pretrained_parts[i+1])\n",
        "                                        break\n",
        "                                    except:\n",
        "                                        pass\n",
        "\n",
        "                            # Check if block indices match and component types match\n",
        "                            if our_block_idx == pretrained_block_idx:\n",
        "                                # Check component type (attn, mlp, norm)\n",
        "                                our_comp = '.'.join(our_parts[our_parts.index('blocks')+2:])\n",
        "                                pretrained_comp = '.'.join(pretrained_parts[pretrained_parts.index('blocks')+2:])\n",
        "                                if our_comp == pretrained_comp:\n",
        "                                    key_mapping[our_key] = pretrained_key\n",
        "                                    found_match = True\n",
        "                                    break\n",
        "                        elif our_parts[-2:] == pretrained_parts[-2:]:\n",
        "                            # For non-block keys, match last 2 parts\n",
        "                            key_mapping[our_key] = pretrained_key\n",
        "                            found_match = True\n",
        "                            break\n",
        "\n",
        "    # Try to load matching weights\n",
        "    loaded_keys = []\n",
        "    missing_keys = []\n",
        "    shape_mismatch_keys = []\n",
        "\n",
        "    for our_key in our_state_dict.keys():\n",
        "        pretrained_key = key_mapping.get(our_key, our_key)\n",
        "\n",
        "        if pretrained_key in pretrained_state_dict:\n",
        "            if our_state_dict[our_key].shape == pretrained_state_dict[pretrained_key].shape:\n",
        "                our_state_dict[our_key] = pretrained_state_dict[pretrained_key]\n",
        "                loaded_keys.append(our_key)\n",
        "            else:\n",
        "                shape_mismatch_keys.append(f\"{our_key} (our: {our_state_dict[our_key].shape} vs pretrained: {pretrained_state_dict[pretrained_key].shape})\")\n",
        "        else:\n",
        "            missing_keys.append(our_key)\n",
        "\n",
        "    # Handle classification head separately\n",
        "    if 'head.weight' in pretrained_state_dict and 'head.bias' in pretrained_state_dict:\n",
        "        pretrained_head_weight = pretrained_state_dict['head.weight']\n",
        "        pretrained_head_bias = pretrained_state_dict['head.bias']\n",
        "\n",
        "        if num_classes == 1000:\n",
        "            if 'head.weight' not in loaded_keys:\n",
        "                our_state_dict['head.weight'] = pretrained_head_weight\n",
        "                our_state_dict['head.bias'] = pretrained_head_bias\n",
        "                loaded_keys.extend(['head.weight', 'head.bias'])\n",
        "        else:\n",
        "            print(f\"Note: Re-initializing head for {num_classes} classes (pretrained had {pretrained_head_weight.shape[0]})\")\n",
        "\n",
        "    # Load the state dict\n",
        "    model.load_state_dict(our_state_dict, strict=False)\n",
        "\n",
        "    print(f\"\\nSuccessfully loaded {len(loaded_keys)} pretrained layers\")\n",
        "    if missing_keys:\n",
        "        print(f\"Warning: {len(missing_keys)} keys not found in pretrained model (first 5): {missing_keys[:5]}\")\n",
        "    if shape_mismatch_keys:\n",
        "        print(f\"Warning: {len(shape_mismatch_keys)} layers have shape mismatches (first 3):\")\n",
        "        for mismatch in shape_mismatch_keys[:3]:\n",
        "            print(f\"  {mismatch}\")\n",
        "\n",
        "    # Critical check: verify key layers were loaded\n",
        "    critical_keys = ['patch_embed.proj.weight', 'pos_embed', 'cls_token']\n",
        "    print(f\"\\nCritical layer check:\")\n",
        "    for key in critical_keys:\n",
        "        if key in loaded_keys:\n",
        "            print(f\"  ✓ Loaded: {key}\")\n",
        "        else:\n",
        "            print(f\"  ✗ WARNING: NOT loaded: {key}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_dataloader(batch_size=32, num_workers=2, img_size=224, train=False):\n",
        "    \"\"\"Get DataLoader for CIFAR-10\"\"\"\n",
        "\n",
        "    # Transform: Resize to 224x224 for ViT, ImageNet normalization\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    root = './data'\n",
        "    dataset = datasets.CIFAR10(root=root, train=train, download=True, transform=transform)\n",
        "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return loader, classes\n",
        "\n",
        "def evaluate_model(model, data_loader, device, classes):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader, desc='Evaluating'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(f'Accuracy: {acc:.2f}%')\n",
        "\n",
        "    # Additional diagnostic info\n",
        "    if acc < 15.0:  # Very low accuracy\n",
        "        print(f'\\n⚠️  WARNING: Very low accuracy ({acc:.2f}%)!')\n",
        "        print('This is likely because:')\n",
        "        print('1. Classification head is randomly initialized (expected if num_classes != 1000)')\n",
        "        print('2. Model needs fine-tuning on CIFAR10 before evaluation')\n",
        "        print('3. Random guessing would give ~10% accuracy on CIFAR10')\n",
        "        print('\\nTo fix: You need to fine-tune the model on CIFAR10 training set first!')\n",
        "\n",
        "    return acc\n",
        "\n",
        "def verify_weight_loading(model, timm_model_name):\n",
        "    \"\"\"Verify that critical weights were loaded correctly\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"Weight Loading Verification\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Load timm model for comparison\n",
        "    pretrained_model = timm.create_model(timm_model_name, pretrained=True, num_classes=1000)\n",
        "    pretrained_state_dict = pretrained_model.state_dict()\n",
        "    our_state_dict = model.state_dict()\n",
        "\n",
        "    # Check critical layers\n",
        "    critical_layers = {\n",
        "        'patch_embed.proj.weight': 'Patch embedding projection',\n",
        "        'pos_embed': 'Positional embeddings',\n",
        "        'cls_token': 'Class token',\n",
        "        'norm.weight': 'Final layer norm weight',\n",
        "        'blocks.0.norm1.weight': 'First transformer block norm'\n",
        "    }\n",
        "\n",
        "    print(\"\\nCritical layer verification:\")\n",
        "    all_loaded = True\n",
        "    for key, description in critical_layers.items():\n",
        "        if key in our_state_dict and key in pretrained_state_dict:\n",
        "            our_weight = our_state_dict[key]\n",
        "            pretrained_weight = pretrained_state_dict[key]\n",
        "\n",
        "            if torch.allclose(our_weight, pretrained_weight, atol=1e-5):\n",
        "                print(f\"  ✓ {description} ({key}): CORRECTLY LOADED\")\n",
        "            else:\n",
        "                print(f\"  ✗ {description} ({key}): MISMATCH!\")\n",
        "                print(f\"    Our: {our_weight.shape}, Pretrained: {pretrained_weight.shape}\")\n",
        "                all_loaded = False\n",
        "        else:\n",
        "            print(f\"  ✗ {description} ({key}): NOT FOUND\")\n",
        "            all_loaded = False\n",
        "\n",
        "    if all_loaded:\n",
        "        print(\"\\n✓ All critical layers loaded correctly!\")\n",
        "    else:\n",
        "        print(\"\\n✗ Some critical layers may not be loaded correctly!\")\n",
        "\n",
        "    return all_loaded\n",
        "\n",
        "def get_dataloader_with_augmentation(batch_size=32, num_workers=2, img_size=224, train=True):\n",
        "    \"\"\"Get DataLoader with data augmentation for training CIFAR-10\"\"\"\n",
        "\n",
        "    if train:\n",
        "        # Training: use data augmentation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomCrop(img_size, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        # Validation/Test: no augmentation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    root = './data'\n",
        "    dataset = datasets.CIFAR10(root=root, train=train, download=True, transform=transform)\n",
        "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=train,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return loader, classes\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [Train]')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='[Val]')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def fine_tune_model(\n",
        "    model,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    freeze_backbone=False,\n",
        "    device='cuda',\n",
        "    save_path='best_model.pth'\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tune the model on CIFAR10\n",
        "\n",
        "    Args:\n",
        "        model: The VisionTransformer model\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size for training\n",
        "        learning_rate: Learning rate\n",
        "        weight_decay: Weight decay for optimizer\n",
        "        freeze_backbone: If True, only train the classification head\n",
        "        device: Device to train on\n",
        "        save_path: Path to save the best model\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fine-tuning on CIFAR10\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Epochs: {epochs}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "    print(f\"Freeze backbone: {freeze_backbone}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Freeze backbone if requested\n",
        "    if freeze_backbone:\n",
        "        print(\"Freezing backbone, only training classification head...\")\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'head' not in name:\n",
        "                param.requires_grad = False\n",
        "        # Count trainable parameters\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100.*trainable_params/total_params:.2f}%)\")\n",
        "    else:\n",
        "        print(\"Training entire model (end-to-end fine-tuning)...\")\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    print(\"\\nLoading datasets...\")\n",
        "    train_loader, classes = get_dataloader_with_augmentation(\n",
        "        batch_size=batch_size, num_workers=2, train=True\n",
        "    )\n",
        "    val_loader, _ = get_dataloader_with_augmentation(\n",
        "        batch_size=batch_size, num_workers=2, train=False\n",
        "    )\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use different learning rates for backbone and head if not freezing\n",
        "    if freeze_backbone:\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "    else:\n",
        "        # Use smaller LR for backbone, larger for head\n",
        "        backbone_params = []\n",
        "        head_params = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if 'head' in name:\n",
        "                    head_params.append(param)\n",
        "                else:\n",
        "                    backbone_params.append(param)\n",
        "\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': backbone_params, 'lr': learning_rate * 0.1},  # Smaller LR for pretrained\n",
        "            {'params': head_params, 'lr': learning_rate}  # Normal LR for new head\n",
        "        ], weight_decay=weight_decay)\n",
        "        print(f\"Using different LRs: backbone={learning_rate*0.1}, head={learning_rate}\")\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=epochs, eta_min=learning_rate * 0.01\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_acc = 0.0\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    print(f\"\\nStarting training...\\n\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch}/{epochs} Summary:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'train_acc': train_acc,\n",
        "            }, save_path)\n",
        "            print(f\"  ✓ Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "        'best_val_acc': best_val_acc\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2NVvVZqmjUC"
      },
      "source": [
        "## Fine-tuning on CIFAR10\n",
        "\n",
        "Fine-tune the pretrained ViT model on CIFAR10. You can choose to:\n",
        "- **Freeze backbone**: Only train the classification head (faster, less memory)\n",
        "- **End-to-end**: Train the entire model (slower, better performance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rG9z0vc9mjUD",
        "outputId": "fa6ffc7d-a3c0-4ce9-b86d-ef51af72feb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b18eb38eee146e6a7436bc1ca5e24fc",
            "687394050d3545e495d9173c6de08517",
            "47847d89a0a14027aa1e7d2b041ed436",
            "75aa10987fb1472cbebf3ddf665eeb42",
            "0181750ae94c4825b681318a315cbd47",
            "43897f704d6546e19ba1ab70b2145cef",
            "511e86e733d944ca8c8a96aee2b631a4",
            "9cab00497eaf4120acc92d039eba7959",
            "ca29ae7f782e433ba9390b94eaebb359",
            "8550d93153cb4dedb56568de4f6d5576",
            "e281171f8cd94aa3a7b36aa9a8c2fc45"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Creating model vit_base_patch16_224 for 10 classes...\n",
            "Loading pretrained weights from timm: vit_base_patch16_224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b18eb38eee146e6a7436bc1ca5e24fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Debug: Our model has 152 parameters\n",
            "Debug: Pretrained model has 152 parameters\n",
            "Debug: Sample our keys (first 10):\n",
            "  1. cls_token\n",
            "  2. pos_embed\n",
            "  3. patch_embed.proj.weight\n",
            "  4. patch_embed.proj.bias\n",
            "  5. blocks.0.norm1.weight\n",
            "  6. blocks.0.norm1.bias\n",
            "  7. blocks.0.attn.qkv.weight\n",
            "  8. blocks.0.attn.qkv.bias\n",
            "  9. blocks.0.attn.proj.weight\n",
            "  10. blocks.0.attn.proj.bias\n",
            "Debug: Sample pretrained keys (first 10):\n",
            "  1. cls_token\n",
            "  2. pos_embed\n",
            "  3. patch_embed.proj.weight\n",
            "  4. patch_embed.proj.bias\n",
            "  5. blocks.0.norm1.weight\n",
            "  6. blocks.0.norm1.bias\n",
            "  7. blocks.0.attn.qkv.weight\n",
            "  8. blocks.0.attn.qkv.bias\n",
            "  9. blocks.0.attn.proj.weight\n",
            "  10. blocks.0.attn.proj.bias\n",
            "Note: Re-initializing head for 10 classes (pretrained had 1000)\n",
            "\n",
            "Successfully loaded 150 pretrained layers\n",
            "Warning: 2 layers have shape mismatches (first 3):\n",
            "  head.weight (our: torch.Size([10, 768]) vs pretrained: torch.Size([1000, 768]))\n",
            "  head.bias (our: torch.Size([10]) vs pretrained: torch.Size([1000]))\n",
            "\n",
            "Critical layer check:\n",
            "  ✓ Loaded: patch_embed.proj.weight\n",
            "  ✓ Loaded: pos_embed\n",
            "  ✓ Loaded: cls_token\n",
            "\n",
            "============================================================\n",
            "Fine-tuning on CIFAR10\n",
            "============================================================\n",
            "Epochs: 10\n",
            "Batch size: 64\n",
            "Learning rate: 0.0001\n",
            "Freeze backbone: False\n",
            "============================================================\n",
            "\n",
            "Training entire model (end-to-end fine-tuning)...\n",
            "Trainable parameters: 85,806,346\n",
            "\n",
            "Loading datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using different LRs: backbone=1e-05, head=0.0001\n",
            "\n",
            "Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 782/782 [27:45<00:00,  2.13s/it, loss=0.0996, acc=96.32%]\n",
            "[Val]: 100%|██████████| 157/157 [02:04<00:00,  1.27it/s, loss=0.0079, acc=98.24%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10 Summary:\n",
            "  Train Loss: 0.1230, Train Acc: 96.32%\n",
            "  Val Loss: 0.0568, Val Acc: 98.24%\n",
            "  Learning Rate: 0.000010\n",
            "  ✓ Saved best model (Val Acc: 98.24%)\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]:  30%|███       | 237/782 [08:34<19:42,  2.17s/it, loss=0.0108, acc=99.20%]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1634591840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Fine-tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m history = fine_tune_model(\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12171590.py\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model, epochs, batch_size, learning_rate, weight_decay, freeze_backbone, device, save_path)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12171590.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Fine-tuning Configuration\n",
        "model_name = 'vit_base_patch16_224'\n",
        "num_classes = 10  # CIFAR10 has 10 classes\n",
        "\n",
        "# Training hyperparameters\n",
        "epochs = 10              # Number of training epochs\n",
        "batch_size = 64           # Batch size (adjust based on GPU memory)\n",
        "learning_rate = 1e-4      # Learning rate\n",
        "weight_decay = 0.01       # Weight decay for regularization\n",
        "freeze_backbone = False   # Set to True to only train classification head\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model\n",
        "print(f\"\\nCreating model {model_name} for {num_classes} classes...\")\n",
        "model = VisionTransformer(\n",
        "    img_size=224,\n",
        "    patch_size=16,\n",
        "    in_channels=3,\n",
        "    num_classes=num_classes,\n",
        "    embed_dim=768,\n",
        "    depth=12,\n",
        "    num_heads=12,\n",
        "    mlp_ratio=4.0\n",
        ")\n",
        "\n",
        "# Load pretrained weights\n",
        "model = load_pretrained_weights(model, model_name, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Fine-tune the model\n",
        "history = fine_tune_model(\n",
        "    model=model,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    freeze_backbone=freeze_backbone,\n",
        "    device=device,\n",
        "    save_path='vit_cifar10_best.pth'\n",
        ")\n",
        "\n",
        "print(\"Fine-tuning completed!\")\n",
        "print(f\"Best validation accuracy: {history['best_val_acc']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qDYNKUvmjUD"
      },
      "source": [
        "## Evaluate Fine-tuned Model\n",
        "\n",
        "Load the best model and evaluate on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9FGYdet9mjUD",
        "outputId": "89a501bc-5949-4571-aa97-6d2b5edea8e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint from vit_cifar10_best.pth...\n",
            "Loaded model from epoch 1\n",
            "Validation accuracy when saved: 98.24%\n",
            "\n",
            "============================================================\n",
            "Evaluating on test set...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [02:04<00:00,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.24%\n",
            "\n",
            "Final test accuracy: 98.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the best model and evaluate\n",
        "checkpoint_path = 'vit_cifar10_best.pth'\n",
        "num_classes = 10  # CIFAR10 has 10 classes\n",
        "batch_size = 64   # Batch size for evaluation\n",
        "\n",
        "# Create model (same architecture)\n",
        "model = VisionTransformer(\n",
        "    img_size=224,\n",
        "    patch_size=16,\n",
        "    in_channels=3,\n",
        "    num_classes=num_classes,\n",
        "    embed_dim=768,\n",
        "    depth=12,\n",
        "    num_heads=12,\n",
        "    mlp_ratio=4.0\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load checkpoint\n",
        "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Validation accuracy when saved: {checkpoint['val_acc']:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Evaluating on test set...\")\n",
        "print(f\"{'='*60}\")\n",
        "test_loader, classes = get_dataloader(batch_size=batch_size, num_workers=2, train=False)\n",
        "test_acc = evaluate_model(model, test_loader, device, classes)\n",
        "print(f\"\\nFinal test accuracy: {test_acc:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b18eb38eee146e6a7436bc1ca5e24fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_687394050d3545e495d9173c6de08517",
              "IPY_MODEL_47847d89a0a14027aa1e7d2b041ed436",
              "IPY_MODEL_75aa10987fb1472cbebf3ddf665eeb42"
            ],
            "layout": "IPY_MODEL_0181750ae94c4825b681318a315cbd47"
          }
        },
        "687394050d3545e495d9173c6de08517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43897f704d6546e19ba1ab70b2145cef",
            "placeholder": "​",
            "style": "IPY_MODEL_511e86e733d944ca8c8a96aee2b631a4",
            "value": "model.safetensors: 100%"
          }
        },
        "47847d89a0a14027aa1e7d2b041ed436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cab00497eaf4120acc92d039eba7959",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca29ae7f782e433ba9390b94eaebb359",
            "value": 346284714
          }
        },
        "75aa10987fb1472cbebf3ddf665eeb42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8550d93153cb4dedb56568de4f6d5576",
            "placeholder": "​",
            "style": "IPY_MODEL_e281171f8cd94aa3a7b36aa9a8c2fc45",
            "value": " 346M/346M [00:03&lt;00:00, 246MB/s]"
          }
        },
        "0181750ae94c4825b681318a315cbd47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43897f704d6546e19ba1ab70b2145cef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511e86e733d944ca8c8a96aee2b631a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cab00497eaf4120acc92d039eba7959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca29ae7f782e433ba9390b94eaebb359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8550d93153cb4dedb56568de4f6d5576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e281171f8cd94aa3a7b36aa9a8c2fc45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}