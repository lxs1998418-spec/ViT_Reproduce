{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Replication Project\n",
    "\n",
    "This notebook allows you to run the Vision Transformer (ViT) replication project on Google Colab.\n",
    "\n",
    "## Setup\n",
    "First, we need to ensure we have the necessary dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install timm tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "We define the Vision Transformer architecture components: PatchEmbedding, MultiHeadAttention, MLP, TransformerBlock, and the main VisionTransformer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image into patch embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # create patch embeddings by conv2d\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch_size, Channel, Height, Width] -> [Batch_size, embed_dim, H', W']\n",
    "        x = self.proj(x) \n",
    "        Batch_size, Channel, Height, Width = x.shape\n",
    "        # [Batch_size, embed_dim, H', W'] -> [Batch_size, num_patches, embed_dim]\n",
    "        x = x.flatten(2).transpose(1, 2)  \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention\"\"\"\n",
    "    \n",
    "    # embed_dim must be divisible by num_heads\n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch_size, 197, 768]\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        # [Batch_size, 197, 768] -> [Batch_size, 197, 3, 12, 64] -> [3, Batch_size, 12, 197, 64]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        # [3, Batch_size, 12, 197, 64] -> [Batch_size, 12, 197, 64]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        # [Batch_size, 12, 197, 64] -> [Batch_size, 12, 197, 197]\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # transfer attention to values\n",
    "        # [Batch_size, 12, 197, 197] -> [Batch_size, 197, 768]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward network\n",
    "    embed_dim -> hidden_dim(expand mlp_ratio times) -> embed_dim\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Block Only Encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch_size, num_patches+1, embed_dim]\n",
    "        # residual connection\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # residual connection\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer Model\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.0,\n",
    "        emb_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Patch embedding [Batch_size, num_patches, embed_dim]\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Learnable class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embedding \n",
    "        # num_patches + 1  including the class token\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Dropout for embeddings\n",
    "        self.pos_dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.normal_(m.bias, std=1e-6)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        # [Batch_size, Channel, Height, Width] -> [Batch_size, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  \n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(Batch_size, -1, -1)  # [Batch_size, 1, embed_dim]\n",
    "        \n",
    "        # [Batch_size, num_patches, embed_dim] -> [Batch_size, num_patches+1, embed_dim]\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  \n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        # [Batch_size, num_patches+1, embed_dim] -> [Batch_size, num_patches+1, embed_dim]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # use class tokens for classification\n",
    "        # [Batch_size, num_patches+1, embed_dim] -> [Batch_size, embed_dim]\n",
    "        cls_token_final = x[:, 0]  \n",
    "\n",
    "        # [Batch_size, embed_dim] -> [Batch_size, num_classes]\n",
    "        class_vectors = self.head(cls_token_final)  \n",
    "        \n",
    "        return class_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Functions for loading pretrained weights and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, timm_model_name, num_classes):\n",
    "    \"\"\"Load pretrained weights from timm library\"\"\"\n",
    "    print(f\"Loading pretrained weights from timm: {timm_model_name}\")\n",
    "    \n",
    "    # Load pretrained model from timm\n",
    "    pretrained_model = timm.create_model(timm_model_name, pretrained=True, num_classes=1000)\n",
    "    \n",
    "    # Get state dicts\n",
    "    our_state_dict = model.state_dict()\n",
    "    pretrained_state_dict = pretrained_model.state_dict()\n",
    "    \n",
    "    # Debug: Print key names to understand the structure\n",
    "    print(f\"\\nDebug: Our model has {len(our_state_dict)} parameters\")\n",
    "    print(f\"Debug: Pretrained model has {len(pretrained_state_dict)} parameters\")\n",
    "    print(f\"Debug: Sample our keys (first 10):\")\n",
    "    for i, key in enumerate(list(our_state_dict.keys())[:10]):\n",
    "        print(f\"  {i+1}. {key}\")\n",
    "    print(f\"Debug: Sample pretrained keys (first 10):\")\n",
    "    for i, key in enumerate(list(pretrained_state_dict.keys())[:10]):\n",
    "        print(f\"  {i+1}. {key}\")\n",
    "    \n",
    "    # Create a mapping from our keys to pretrained keys\n",
    "    # timm ViT models typically use the same structure, so keys should match\n",
    "    key_mapping = {}\n",
    "    for our_key in our_state_dict.keys():\n",
    "        # Try direct match first (most common case)\n",
    "        if our_key in pretrained_state_dict:\n",
    "            key_mapping[our_key] = our_key\n",
    "        else:\n",
    "            # If direct match fails, try to find the corresponding key\n",
    "            # This handles cases where timm might use slightly different naming\n",
    "            found_match = False\n",
    "            \n",
    "            # Extract the key parts\n",
    "            our_parts = our_key.split('.')\n",
    "            \n",
    "            # Try to find matching key in pretrained model\n",
    "            for pretrained_key in pretrained_state_dict.keys():\n",
    "                pretrained_parts = pretrained_key.split('.')\n",
    "                \n",
    "                # Match if the last parts are the same and structure is similar\n",
    "                if len(our_parts) == len(pretrained_parts):\n",
    "                    # Check if the last 2-3 parts match (handles blocks.0.attn vs blocks.0.attn)\n",
    "                    if our_parts[-1] == pretrained_parts[-1]:\n",
    "                        # For blocks, also check the layer number and component type\n",
    "                        if 'blocks' in our_key and 'blocks' in pretrained_key:\n",
    "                            # Extract block number and component\n",
    "                            our_block_idx = None\n",
    "                            pretrained_block_idx = None\n",
    "                            for i, part in enumerate(our_parts):\n",
    "                                if part == 'blocks' and i+1 < len(our_parts):\n",
    "                                    try:\n",
    "                                        our_block_idx = int(our_parts[i+1])\n",
    "                                        break\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            for i, part in enumerate(pretrained_parts):\n",
    "                                if part == 'blocks' and i+1 < len(pretrained_parts):\n",
    "                                    try:\n",
    "                                        pretrained_block_idx = int(pretrained_parts[i+1])\n",
    "                                        break\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            \n",
    "                            # Check if block indices match and component types match\n",
    "                            if our_block_idx == pretrained_block_idx:\n",
    "                                # Check component type (attn, mlp, norm)\n",
    "                                our_comp = '.'.join(our_parts[our_parts.index('blocks')+2:])\n",
    "                                pretrained_comp = '.'.join(pretrained_parts[pretrained_parts.index('blocks')+2:])\n",
    "                                if our_comp == pretrained_comp:\n",
    "                                    key_mapping[our_key] = pretrained_key\n",
    "                                    found_match = True\n",
    "                                    break\n",
    "                        elif our_parts[-2:] == pretrained_parts[-2:]:\n",
    "                            # For non-block keys, match last 2 parts\n",
    "                            key_mapping[our_key] = pretrained_key\n",
    "                            found_match = True\n",
    "                            break\n",
    "    \n",
    "    # Try to load matching weights\n",
    "    loaded_keys = []\n",
    "    missing_keys = []\n",
    "    shape_mismatch_keys = []\n",
    "    \n",
    "    for our_key in our_state_dict.keys():\n",
    "        pretrained_key = key_mapping.get(our_key, our_key)\n",
    "        \n",
    "        if pretrained_key in pretrained_state_dict:\n",
    "            if our_state_dict[our_key].shape == pretrained_state_dict[pretrained_key].shape:\n",
    "                our_state_dict[our_key] = pretrained_state_dict[pretrained_key]\n",
    "                loaded_keys.append(our_key)\n",
    "            else:\n",
    "                shape_mismatch_keys.append(f\"{our_key} (our: {our_state_dict[our_key].shape} vs pretrained: {pretrained_state_dict[pretrained_key].shape})\")\n",
    "        else:\n",
    "            missing_keys.append(our_key)\n",
    "    \n",
    "    # Handle classification head separately\n",
    "    if 'head.weight' in pretrained_state_dict and 'head.bias' in pretrained_state_dict:\n",
    "        pretrained_head_weight = pretrained_state_dict['head.weight']\n",
    "        pretrained_head_bias = pretrained_state_dict['head.bias']\n",
    "        \n",
    "        if num_classes == 1000:\n",
    "            if 'head.weight' not in loaded_keys:\n",
    "                our_state_dict['head.weight'] = pretrained_head_weight\n",
    "                our_state_dict['head.bias'] = pretrained_head_bias\n",
    "                loaded_keys.extend(['head.weight', 'head.bias'])\n",
    "        else:\n",
    "            print(f\"Note: Re-initializing head for {num_classes} classes (pretrained had {pretrained_head_weight.shape[0]})\")\n",
    "    \n",
    "    # Load the state dict\n",
    "    model.load_state_dict(our_state_dict, strict=False)\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(loaded_keys)} pretrained layers\")\n",
    "    if missing_keys:\n",
    "        print(f\"Warning: {len(missing_keys)} keys not found in pretrained model (first 5): {missing_keys[:5]}\")\n",
    "    if shape_mismatch_keys:\n",
    "        print(f\"Warning: {len(shape_mismatch_keys)} layers have shape mismatches (first 3):\")\n",
    "        for mismatch in shape_mismatch_keys[:3]:\n",
    "            print(f\"  {mismatch}\")\n",
    "    \n",
    "    # Critical check: verify key layers were loaded\n",
    "    critical_keys = ['patch_embed.proj.weight', 'pos_embed', 'cls_token']\n",
    "    print(f\"\\nCritical layer check:\")\n",
    "    for key in critical_keys:\n",
    "        if key in loaded_keys:\n",
    "            print(f\"  ✓ Loaded: {key}\")\n",
    "        else:\n",
    "            print(f\"  ✗ WARNING: NOT loaded: {key}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_dataloader(dataset_name='cifar10', batch_size=32, num_workers=2, img_size=224, train=False):\n",
    "    \"\"\"Get DataLoader for CIFAR-10 or CIFAR-100\"\"\"\n",
    "    \n",
    "    # Transform: Resize to 224x224 for ViT, ImageNet normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    root = './data'\n",
    "    if dataset_name.lower() == 'cifar10':\n",
    "        dataset = datasets.CIFAR10(root=root, train=train, download=True, transform=transform)\n",
    "        classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    elif dataset_name.lower() == 'cifar100':\n",
    "        dataset = datasets.CIFAR100(root=root, train=train, download=True, transform=transform)\n",
    "        classes = [str(i) for i in range(100)] # CIFAR-100 has 100 classes\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return loader, classes\n",
    "\n",
    "def evaluate_model(model, data_loader, device, classes):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    acc = 100. * correct / total\n",
    "    print(f'Accuracy: {acc:.2f}%')\n",
    "    \n",
    "    # Additional diagnostic info\n",
    "    if acc < 15.0:  # Very low accuracy\n",
    "        print(f'\\n⚠️  WARNING: Very low accuracy ({acc:.2f}%)!')\n",
    "        print('This is likely because:')\n",
    "        print('1. Classification head is randomly initialized (expected if num_classes != 1000)')\n",
    "        print('2. Model needs fine-tuning on CIFAR10 before evaluation')\n",
    "        print('3. Random guessing would give ~10% accuracy on CIFAR10')\n",
    "        print('\\nTo fix: You need to fine-tune the model on CIFAR10 training set first!')\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def verify_weight_loading(model, timm_model_name):\n",
    "    \"\"\"Verify that critical weights were loaded correctly\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Weight Loading Verification\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load timm model for comparison\n",
    "    pretrained_model = timm.create_model(timm_model_name, pretrained=True, num_classes=1000)\n",
    "    pretrained_state_dict = pretrained_model.state_dict()\n",
    "    our_state_dict = model.state_dict()\n",
    "    \n",
    "    # Check critical layers\n",
    "    critical_layers = {\n",
    "        'patch_embed.proj.weight': 'Patch embedding projection',\n",
    "        'pos_embed': 'Positional embeddings',\n",
    "        'cls_token': 'Class token',\n",
    "        'norm.weight': 'Final layer norm weight',\n",
    "        'blocks.0.norm1.weight': 'First transformer block norm'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCritical layer verification:\")\n",
    "    all_loaded = True\n",
    "    for key, description in critical_layers.items():\n",
    "        if key in our_state_dict and key in pretrained_state_dict:\n",
    "            our_weight = our_state_dict[key]\n",
    "            pretrained_weight = pretrained_state_dict[key]\n",
    "            \n",
    "            if torch.allclose(our_weight, pretrained_weight, atol=1e-5):\n",
    "                print(f\"  ✓ {description} ({key}): CORRECTLY LOADED\")\n",
    "            else:\n",
    "                print(f\"  ✗ {description} ({key}): MISMATCH!\")\n",
    "                print(f\"    Our: {our_weight.shape}, Pretrained: {pretrained_weight.shape}\")\n",
    "                all_loaded = False\n",
    "        else:\n",
    "            print(f\"  ✗ {description} ({key}): NOT FOUND\")\n",
    "            all_loaded = False\n",
    "    \n",
    "    if all_loaded:\n",
    "        print(\"\\n✓ All critical layers loaded correctly!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Some critical layers may not be loaded correctly!\")\n",
    "    \n",
    "    return all_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Run the evaluation on CIFAR-10 and/or CIFAR-100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题诊断\n",
    "\n",
    "如果准确率很低（如7-10%），可能的原因：\n",
    "\n",
    "1. **分类头随机初始化**：当类别数从1000变为10时，分类头被重新初始化。**必须进行微调（fine-tuning）才能获得好的性能**。\n",
    "\n",
    "2. **权重加载不完整**：检查上面的调试输出，确认关键层（patch_embed, pos_embed, cls_token）是否正确加载。\n",
    "\n",
    "3. **模型未设置为评估模式**：确保在评估时调用 `model.eval()`。\n",
    "\n",
    "**解决方案**：\n",
    "- 如果只是测试预训练模型，应该使用 `num_classes=1000` 并在ImageNet上测试\n",
    "- 如果要在CIFAR10上使用，**必须进行微调训练**，不能直接评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dataset_name = 'cifar10'  # Options: 'cifar10', 'cifar100', 'all'\n",
    "batch_size = 64           # Adjust based on Colab GPU memory (e.g. 32, 64, 128)\n",
    "model_name = 'vit_base_patch16_224'\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Determine datasets to evaluate\n",
    "if dataset_name == 'all':\n",
    "    datasets_to_eval = ['cifar10', 'cifar100']\n",
    "else:\n",
    "    datasets_to_eval = [dataset_name]\n",
    "\n",
    "for ds_name in datasets_to_eval:\n",
    "    print(f\"\\n{'='*20} Evaluating on {ds_name.upper()} {'='*20}\")\n",
    "    \n",
    "    if ds_name == 'cifar10':\n",
    "        num_classes = 10\n",
    "    else:\n",
    "        num_classes = 100\n",
    "        \n",
    "    # Create model\n",
    "    print(f\"Creating model {model_name} for {num_classes} classes...\")\n",
    "    model = VisionTransformer(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model = load_pretrained_weights(model, model_name, num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"Loading {ds_name} test set...\")\n",
    "    # Ensure we're running with appropriate num_workers for Colab\n",
    "    loader, classes = get_dataloader(ds_name, batch_size=batch_size, num_workers=2, train=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate_model(model, loader, device, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
